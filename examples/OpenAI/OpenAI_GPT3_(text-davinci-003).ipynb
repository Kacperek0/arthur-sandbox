{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794518f6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Arthur Sandbox Example: OpenAI question-answering\n",
    "\n",
    "In this guide, we'll use a question-answering dataset from Huggingface and the GPT3 endpoint from OpenAI to onboard a new streaming model to the Arthur platform. Then we will use Arthur to analyze our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e524a952",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Outline\n",
    "\n",
    "Read on for an overview of everything this notebook will cover. **[Click here to dive straight into the code.](#Imports)**\n",
    "\n",
    "## Onboarding\n",
    "\n",
    "Onboarding is the process of setting up your model to be monitored by Arthur. You specify the type of data your model ingests, send a reference dataset to provide a baseline of the distribution of your data, and you configure additional settings among the services Arthur offers.\n",
    "\n",
    "**Arthur does not need your model object itself to monitor performance - only predictions are required**\n",
    "\n",
    "All you need to monitor your model with Arthur is to upload the predictions your model makes: Arthur computes analytics about your model based on that prediction data. This data can be computed directly by your model in a script or notebook like this one to be uploaded to the platform, or can be fetched from an external database to be sent to Arthur.\n",
    "\n",
    "### Getting Model Predictions\n",
    "We'll prepare a sample from a question-answering dataset and generate answers from the GPT-3 endpoint.\n",
    "\n",
    "### Registering Model with Arthur\n",
    "We'll configure our model's attributes and save the model to the Arthur platform.\n",
    "\n",
    "### Sending Inferences\n",
    "We'll send model inferences (inputs and predictions) to the Arthur platform.\n",
    "\n",
    "\n",
    "## Model Monitoring and Analysis\n",
    "\n",
    "Once onboarding is complete and you have inferences uploaded to the platform, you can use Arthur to get model monitoring insights.\n",
    "\n",
    "We will evaluate the Arthur-computed metrics to identify trends in our model's inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6bae60",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ebea11",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fecba162",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ensure required packages are installed\n",
    "#  don't worry, our requirements are flexible!\n",
    "\n",
    "! pip install -r requirements.txt > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e6b33b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:08:14.390559Z",
     "start_time": "2021-09-02T19:08:12.824796Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shortuuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61671d47",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9ce8a1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Onboarding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac8075",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "We're using the [SciQ dataset from Huggingface](https://huggingface.co/datasets/sciq)\n",
    "\n",
    "The SciQ dataset contains 13,679 crowdsourced science exam questions about Physics, Chemistry and Biology, among others. The questions are in multiple-choice format with 4 answer options each. For the majority of the questions, an additional paragraph with supporting evidence for the correct answer is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cefa1fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6be9d660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sciq (/Users/maxcembalest/.cache/huggingface/datasets/sciq/default/0.1.0/50e5c6e3795b55463819d399ec417bfd4c3c621105e00295ddb5f3633d708493)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920a4f5abd594bec84b99fcfe7a2ccd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sciq_dataset = load_dataset(\"sciq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c71cc7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sciq_df = sciq_dataset.data['train'].table.to_pandas()\n",
    "sciq_df_inf = sciq_dataset.data['test'].table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3408ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sciq_df_sample = sciq_df.sample(5)\n",
    "sciq_df_inf_sample = sciq_df_inf.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585dc8ae",
   "metadata": {},
   "source": [
    "## Create the full LLM inputs by concatenating the actual question after a sentence/paragraph of supporting information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "798a8a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_inputs = [\n",
    "    f\"What is the answer to this question? question: {row.question}, answer: \" \n",
    "    for _, row in sciq_df_sample.iterrows()\n",
    "]\n",
    "\n",
    "gpt3_inputs_inf = [\n",
    "    f\"What is the answer to this question? question: {row.question}, answer: \" \n",
    "    for _, row in sciq_df_inf_sample.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bd03e2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the answer to this question? question: In binary fission how many times does a cell split?, answer: ',\n",
       " 'What is the answer to this question? question: What is the movement of muscle in the digestive system called?, answer: ',\n",
       " 'What is the answer to this question? question: What is the basic unit of structure and function of living things?, answer: ',\n",
       " 'What is the answer to this question? question: Use of oil-consuming bacteria to clean up an oil spill is an example of what?, answer: ',\n",
       " 'What is the answer to this question? question: What is it called when the chance that a certain event will occur?, answer: ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt3_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0cc709",
   "metadata": {},
   "source": [
    "# Get OpenAI responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4433e425",
   "metadata": {},
   "source": [
    "Using the endpoint for the `text-davinci-003` model, we get an answer to each question from our sample dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3abb0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3793cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def get_gpt3_responses(inputs: List[str]):\n",
    "    \"\"\"\n",
    "    Applies GPT3 to the inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    responses = []\n",
    "    \n",
    "    for inp in inputs:\n",
    "        openai_response = openai.Completion.create(\n",
    "            model=\"text-davinci-003\", \n",
    "            prompt=inp, \n",
    "            temperature=0.1, \n",
    "            logprobs=1)\n",
    "        gpt3_answer = openai_response[\"choices\"][0][\"text\"]\n",
    "        responses.append(openai_response)\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81ab906a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "responses = get_gpt3_responses(gpt3_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcc2d7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_inf = get_gpt3_responses(gpt3_inputs_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bde20ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_response_texts = [c['choices'][0]['text'] for c in responses]\n",
    "gpt3_response_texts_inf = [c['choices'][0]['text'] for c in responses_inf]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b7286a",
   "metadata": {},
   "source": [
    "#### Get token likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "966fc8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_tokens_ref = [resp['choices'][0]['logprobs']['tokens'] for resp in responses]\n",
    "gpt3_probabilities_ref = [list(np.exp(resp['choices'][0]['logprobs']['token_logprobs'])) for resp in responses]\n",
    "gpt3_token_likelihoods_ref = [[\n",
    "    {t: p}\n",
    "    for t, p in zip(gpt3_tokens_ref[i], gpt3_probabilities_ref[i])] for i in range(len(gpt3_inputs))]\n",
    "\n",
    "gpt3_tokens_inf = [resp['choices'][0]['logprobs']['tokens'] for resp in responses_inf]\n",
    "gpt3_probabilities_inf = [list(np.exp(resp['choices'][0]['logprobs']['token_logprobs'])) for resp in responses_inf]\n",
    "gpt3_token_likelihoods_inf = [[\n",
    "    {t: p}\n",
    "    for t, p in zip(gpt3_tokens_inf[i], gpt3_probabilities_inf[i])] for i in range(len(gpt3_inputs_inf))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669a2d10",
   "metadata": {},
   "source": [
    "#### Get finish reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5553d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_response_finish_reasons_ref = [c['choices'][0]['finish_reason'] for c in responses]\n",
    "gpt3_response_finish_reasons_inf = [c['choices'][0]['finish_reason'] for c in responses_inf]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9403a19",
   "metadata": {},
   "source": [
    "#### Get readability scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e80bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "gpt3_readability_ref = [textstat.flesch_reading_ease(o) for o in gpt3_response_texts]\n",
    "gpt3_readability_inf = [textstat.flesch_reading_ease(o) for o in gpt3_response_texts_inf]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc301825",
   "metadata": {},
   "source": [
    "#### Get correctness of answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ca7b179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correctness(outputs, answers):\n",
    "    correctnesses = []\n",
    "    for a, o in zip(answers, outputs):\n",
    "        in_ = True\n",
    "        for word in a.replace(',','').split():\n",
    "            if word.lower() not in o.lower():\n",
    "                in_=False\n",
    "        correctnesses.append(in_)\n",
    "    return correctnesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4516fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_correctness_ref = get_correctness(\n",
    "    gpt3_response_texts,\n",
    "    list(sciq_df_sample.correct_answer.values)\n",
    ")\n",
    "\n",
    "gpt3_correctness_inf = get_correctness(\n",
    "    gpt3_response_texts_inf,\n",
    "    list(sciq_df_inf_sample.correct_answer.values)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a5981c",
   "metadata": {},
   "source": [
    "#### Use ChatGPT to get LLM-generated feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "643161ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pseudofeedback(inputs: List[str]):\n",
    "    responses = []\n",
    "    for inp in inputs:\n",
    "        print(inp)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You give short written feedback saying if an answer made sense for a given question. You also rate the answer out of 10 (0=worst,10=best)\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is the answer to this question? question: What makes breathing difficult due to respiratory system disease?, answer: The answer to this question is the answer to your question.\\n\\nIt is important to know the answer to this question because\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"You didn't actually answer the question, thats bad and confusing. 1\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is the answer to this question? question: What makes breathing difficult due to respiratory system disease?, answer: Respiratory system diseases can make breathing difficult by causing inflammation, damage, or blockages in the airways or lung tissues. These can include conditions such as asthma, chronic obstructive pulmonary disease (COPD), pneumonia, lung cancer, and others. The inflammation and swelling can narrow the airways, making it harder for air to flow in and out of the lungs. Damage to the lung tissues can reduce their ability to expand and contract, which can also make breathing difficult. Additionally, excess mucus or fluid in the lungs can further narrow the airways and make it hard to breathe. These factors can cause symptoms such as shortness of breath, coughing, wheezing, and fatigue.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Thank you very much, that makes a lot of sense! 10\"},\n",
    "            {\"role\": \"user\", \"content\": inp}\n",
    "        ]\n",
    "\n",
    "        openai_response = openai.ChatCompletion.create(\n",
    "          model=\"gpt-3.5-turbo\",\n",
    "          messages=messages\n",
    "        )\n",
    "        chat_gpt_answer = openai_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        print(chat_gpt_answer, \"\\n=====\\n\")\n",
    "        responses.append(openai_response)\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d41a9533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the answer to this question? question: In binary fission how many times does a cell split?, answer: \n",
      "\n",
      "In binary fission, a cell splits into two identical daughter cells.\n",
      "Yes, that is correct and makes sense. 10 \n",
      "=====\n",
      "\n",
      "What is the answer to this question? question: What is the movement of muscle in the digestive system called?, answer: \n",
      "\n",
      "Peristalsis.\n",
      "Great, that is a correct and concise answer! 10 \n",
      "=====\n",
      "\n",
      "What is the answer to this question? question: What is the basic unit of structure and function of living things?, answer: \n",
      "\n",
      "Cell\n",
      "Your answer is correct and makes sense. 10 \n",
      "=====\n",
      "\n",
      "What is the answer to this question? question: Use of oil-consuming bacteria to clean up an oil spill is an example of what?, answer: \n",
      "\n",
      "Bioremediation.\n",
      "Great answer, that makes sense! 10 \n",
      "=====\n",
      "\n",
      "What is the answer to this question? question: What is it called when the chance that a certain event will occur?, answer:  Probability.\n",
      "Your answer is correct and makes sense. 10. \n",
      "=====\n",
      "\n",
      "What is the answer to this question? question: What structures are at the end of the long air passages in the lungs?, answer: \n",
      "Alveoli.\n",
      "Your answer is correct and concise. Well done! 10 \n",
      "=====\n",
      "\n",
      "What is the answer to this question? question: What is the most important source of electromagnetic waves on earth?, answer: \n",
      "\n",
      "The Sun.\n",
      "That answer is accurate and makes sense. 9 \n",
      "=====\n",
      "\n",
      "What is the answer to this question? question: Schrödinger’s approach uses three quantum numbers (n, l, and ml) to specify any of what type of function, associated with a particular energy?, answer: \n",
      "\n",
      "Schrödinger's approach uses three quantum numbers (n,\n",
      "l, and ml) to specify the wave function or orbital of an electron in an atom, which is associated with a particular energy level. Therefore, the type of function that is specified by these quantum numbers is the atomic orbital function. \n",
      "=====\n",
      "\n",
      "What is the answer to this question? question: The five human senses are taste, touch, vision, hearing and one more. what is it?, answer:  Smell.\n",
      "Great job! Your answer makes complete sense. 10. \n",
      "=====\n",
      "\n",
      "What is the answer to this question? question: When a nerve impulse reaches the end of an axon, what are the chemicals released by the axon called?, answer: \n",
      "\n",
      "Neurotransmitters.\n",
      "Great, that answer is accurate and concise. 10 \n",
      "=====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt3_pseudofeedback_ref = get_pseudofeedback([inp + rt for inp, rt in zip(gpt3_inputs, gpt3_response_texts)])\n",
    "gpt3_pseudofeedback_inf = get_pseudofeedback([inp + rt for inp, rt in zip(gpt3_inputs_inf, gpt3_response_texts_inf)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2771e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feedback_scores(feedback_strings):\n",
    "    feedback_ints = []\n",
    "    for fs in feedback_strings:\n",
    "        score = -1\n",
    "        fs = fs.replace('out of 10', '')\n",
    "        for num_string in np.arange(11).astype(str):\n",
    "            if num_string in fs:\n",
    "                score = int(num_string)\n",
    "        feedback_ints.append(score)\n",
    "    return feedback_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8900354",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_pseudofeedback_messages_ref = [c['choices'][0]['message']['content'] for c in gpt3_pseudofeedback_ref]\n",
    "gpt3_pseudofeedback_messages_inf = [c['choices'][0]['message']['content'] for c in gpt3_pseudofeedback_inf]\n",
    "\n",
    "gpt3_feedback_scores_ref = get_feedback_scores(gpt3_pseudofeedback_messages_ref)\n",
    "gpt3_feedback_scores_inf = get_feedback_scores(gpt3_pseudofeedback_messages_inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709930f",
   "metadata": {},
   "source": [
    "### Create inference dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "def2953c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>output_text</th>\n",
       "      <th>token_likelihoods</th>\n",
       "      <th>finish_reason</th>\n",
       "      <th>readability</th>\n",
       "      <th>correct</th>\n",
       "      <th>user_feedback</th>\n",
       "      <th>user_feedback_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the answer to this question? question:...</td>\n",
       "      <td>\\n\\nIn binary fission, a cell splits into two ...</td>\n",
       "      <td>[{'\n",
       "': 0.9973323729575256}, {'\n",
       "': 0.9903408105...</td>\n",
       "      <td>length</td>\n",
       "      <td>60.31</td>\n",
       "      <td>True</td>\n",
       "      <td>Yes, that is correct and makes sense. 10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the answer to this question? question:...</td>\n",
       "      <td>\\n\\nPeristalsis.</td>\n",
       "      <td>[{'\n",
       "': 0.8272828525535677}, {'\n",
       "': 0.8514157387...</td>\n",
       "      <td>stop</td>\n",
       "      <td>-47.99</td>\n",
       "      <td>True</td>\n",
       "      <td>Great, that is a correct and concise answer! 10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the answer to this question? question:...</td>\n",
       "      <td>\\n\\nCell</td>\n",
       "      <td>[{'\n",
       "': 0.8079144942571181}, {'\n",
       "': 0.6975522204...</td>\n",
       "      <td>stop</td>\n",
       "      <td>121.22</td>\n",
       "      <td>True</td>\n",
       "      <td>Your answer is correct and makes sense. 10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the answer to this question? question:...</td>\n",
       "      <td>\\n\\nBioremediation.</td>\n",
       "      <td>[{'\n",
       "': 0.458393909546269}, {'\n",
       "': 0.99104760150...</td>\n",
       "      <td>stop</td>\n",
       "      <td>-217.19</td>\n",
       "      <td>True</td>\n",
       "      <td>Great answer, that makes sense! 10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the answer to this question? question:...</td>\n",
       "      <td>Probability.</td>\n",
       "      <td>[{' Prob': 0.7149669937078649}, {'ability': 0....</td>\n",
       "      <td>stop</td>\n",
       "      <td>-217.19</td>\n",
       "      <td>True</td>\n",
       "      <td>Your answer is correct and makes sense. 10.</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What is the answer to this question? question:...   \n",
       "1  What is the answer to this question? question:...   \n",
       "2  What is the answer to this question? question:...   \n",
       "3  What is the answer to this question? question:...   \n",
       "4  What is the answer to this question? question:...   \n",
       "\n",
       "                                         output_text  \\\n",
       "0  \\n\\nIn binary fission, a cell splits into two ...   \n",
       "1                                   \\n\\nPeristalsis.   \n",
       "2                                           \\n\\nCell   \n",
       "3                                \\n\\nBioremediation.   \n",
       "4                                       Probability.   \n",
       "\n",
       "                                   token_likelihoods finish_reason  \\\n",
       "0  [{'\n",
       "': 0.9973323729575256}, {'\n",
       "': 0.9903408105...        length   \n",
       "1  [{'\n",
       "': 0.8272828525535677}, {'\n",
       "': 0.8514157387...          stop   \n",
       "2  [{'\n",
       "': 0.8079144942571181}, {'\n",
       "': 0.6975522204...          stop   \n",
       "3  [{'\n",
       "': 0.458393909546269}, {'\n",
       "': 0.99104760150...          stop   \n",
       "4  [{' Prob': 0.7149669937078649}, {'ability': 0....          stop   \n",
       "\n",
       "   readability  correct                                    user_feedback  \\\n",
       "0        60.31     True         Yes, that is correct and makes sense. 10   \n",
       "1       -47.99     True  Great, that is a correct and concise answer! 10   \n",
       "2       121.22     True       Your answer is correct and makes sense. 10   \n",
       "3      -217.19     True               Great answer, that makes sense! 10   \n",
       "4      -217.19     True      Your answer is correct and makes sense. 10.   \n",
       "\n",
       "   user_feedback_score  \n",
       "0                   10  \n",
       "1                   10  \n",
       "2                   10  \n",
       "3                   10  \n",
       "4                   10  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt3_ref_data = pd.DataFrame({\n",
    "    'user_input': gpt3_inputs,\n",
    "    'output_text': gpt3_response_texts,\n",
    "    'token_likelihoods': gpt3_token_likelihoods_ref,\n",
    "    'finish_reason': gpt3_response_finish_reasons_ref,\n",
    "    'readability': gpt3_readability_ref,\n",
    "    'correct': gpt3_correctness_ref,\n",
    "    'user_feedback': gpt3_pseudofeedback_messages_ref,\n",
    "    'user_feedback_score': gpt3_feedback_scores_ref\n",
    "})\n",
    "gpt3_ref_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92dcae38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>output_text</th>\n",
       "      <th>token_likelihoods</th>\n",
       "      <th>finish_reason</th>\n",
       "      <th>readability</th>\n",
       "      <th>correct</th>\n",
       "      <th>user_feedback</th>\n",
       "      <th>user_feedback_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the answer to this question? question:...</td>\n",
       "      <td>\\nAlveoli.</td>\n",
       "      <td>[{'\n",
       "': 0.6886342436057409}, {'Al': 0.814359430...</td>\n",
       "      <td>stop</td>\n",
       "      <td>36.62</td>\n",
       "      <td>True</td>\n",
       "      <td>Your answer is correct and concise. Well done! 10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the answer to this question? question:...</td>\n",
       "      <td>\\n\\nThe Sun.</td>\n",
       "      <td>[{'\n",
       "': 0.9694359882575952}, {'\n",
       "': 0.8036643034...</td>\n",
       "      <td>stop</td>\n",
       "      <td>120.21</td>\n",
       "      <td>True</td>\n",
       "      <td>That answer is accurate and makes sense. 9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the answer to this question? question:...</td>\n",
       "      <td>\\n\\nSchrödinger's approach uses three quantum ...</td>\n",
       "      <td>[{'\n",
       "': 0.6784315222521419}, {'\n",
       "': 0.7127359027...</td>\n",
       "      <td>length</td>\n",
       "      <td>64.37</td>\n",
       "      <td>False</td>\n",
       "      <td>l, and ml) to specify the wave function or orb...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the answer to this question? question:...</td>\n",
       "      <td>Smell.</td>\n",
       "      <td>[{' Sm': 0.5371301762378593}, {'ell': 0.999999...</td>\n",
       "      <td>stop</td>\n",
       "      <td>121.22</td>\n",
       "      <td>True</td>\n",
       "      <td>Great job! Your answer makes complete sense. 10.</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the answer to this question? question:...</td>\n",
       "      <td>\\n\\nNeurotransmitters.</td>\n",
       "      <td>[{'\n",
       "': 0.6816356509754551}, {'\n",
       "': 0.9149446750...</td>\n",
       "      <td>stop</td>\n",
       "      <td>-217.19</td>\n",
       "      <td>True</td>\n",
       "      <td>Great, that answer is accurate and concise. 10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What is the answer to this question? question:...   \n",
       "1  What is the answer to this question? question:...   \n",
       "2  What is the answer to this question? question:...   \n",
       "3  What is the answer to this question? question:...   \n",
       "4  What is the answer to this question? question:...   \n",
       "\n",
       "                                         output_text  \\\n",
       "0                                         \\nAlveoli.   \n",
       "1                                       \\n\\nThe Sun.   \n",
       "2  \\n\\nSchrödinger's approach uses three quantum ...   \n",
       "3                                             Smell.   \n",
       "4                             \\n\\nNeurotransmitters.   \n",
       "\n",
       "                                   token_likelihoods finish_reason  \\\n",
       "0  [{'\n",
       "': 0.6886342436057409}, {'Al': 0.814359430...          stop   \n",
       "1  [{'\n",
       "': 0.9694359882575952}, {'\n",
       "': 0.8036643034...          stop   \n",
       "2  [{'\n",
       "': 0.6784315222521419}, {'\n",
       "': 0.7127359027...        length   \n",
       "3  [{' Sm': 0.5371301762378593}, {'ell': 0.999999...          stop   \n",
       "4  [{'\n",
       "': 0.6816356509754551}, {'\n",
       "': 0.9149446750...          stop   \n",
       "\n",
       "   readability  correct                                      user_feedback  \\\n",
       "0        36.62     True  Your answer is correct and concise. Well done! 10   \n",
       "1       120.21     True         That answer is accurate and makes sense. 9   \n",
       "2        64.37    False  l, and ml) to specify the wave function or orb...   \n",
       "3       121.22     True   Great job! Your answer makes complete sense. 10.   \n",
       "4      -217.19     True     Great, that answer is accurate and concise. 10   \n",
       "\n",
       "   user_feedback_score  \n",
       "0                   10  \n",
       "1                    9  \n",
       "2                   -1  \n",
       "3                   10  \n",
       "4                   10  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt3_inf_data = pd.DataFrame({\n",
    "    'user_input': gpt3_inputs_inf,\n",
    "    'output_text': gpt3_response_texts_inf,\n",
    "    'token_likelihoods': gpt3_token_likelihoods_inf,\n",
    "    'finish_reason': gpt3_response_finish_reasons_inf,\n",
    "    'readability': gpt3_readability_inf,\n",
    "    'correct': gpt3_correctness_inf,\n",
    "    'user_feedback': gpt3_pseudofeedback_messages_inf,\n",
    "    'user_feedback_score': gpt3_feedback_scores_inf\n",
    "})\n",
    "gpt3_inf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "155ff30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt3_ref_data.to_csv('gpt3_reference_data.csv')\n",
    "# gpt3_inf_data.to_csv('gpt3_inference_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bacde00",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Registering Model With Arthur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8204e0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Setting Up Connection\n",
    "Supply your login to authenticate with the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d0ffb32",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from arthurai import ArthurAI\n",
    "# connect to Arthur\n",
    "# UNCOMMENT the two lines below and enter your details\n",
    "arthur = ArthurAI(\n",
    "    url=\"https://app.arthur.ai\",  # you can also pass this through the ARTHUR_ENDPOINT_URL environment variable\n",
    "    login=\"<YOUR LOGIN HERE>\",  # you can also pass this through the ARTHUR_LOGIN environment variable\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f45cba",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Registering Model Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc1c77fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arthurai.common.constants import InputType, OutputType, ValueType, Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976dac6d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We'll instantiate an [`ArthurModel`](https://docs.arthur.ai/sdk/sdk_v3/apiref/arthurai.core.models.ArthurModel.html) with the `ArthurAI.model()` method, which constructs a new local `ArthurModel` object. Later we'll use `ArthurModel.save()` to register this model with the Arthur platform.\n",
    "\n",
    "We give the model a user-friendly `display_name` and allow the unique `partner_model_id` field to be automatically generated, but you can supply a unique identifier if it helps you map your models in Arthur to your other MLOps systems.\n",
    "\n",
    "The `InputType` of a model specifies the general type of data your model ingests. The `OutputType` of a model specifies the modeling task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b60f099",
   "metadata": {},
   "source": [
    "### Building the model by specifying attributes\n",
    "\n",
    "We use a helper function to register the model attributes for the input and output text the model will process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c7c14b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:08:20.533507Z",
     "start_time": "2021-09-02T19:08:20.531272Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>stage</th>\n",
       "      <th>value_type</th>\n",
       "      <th>categorical</th>\n",
       "      <th>is_unique</th>\n",
       "      <th>categories</th>\n",
       "      <th>bins</th>\n",
       "      <th>range</th>\n",
       "      <th>monitor_for_bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_input</td>\n",
       "      <td>PIPELINE_INPUT</td>\n",
       "      <td>UNSTRUCTURED_TEXT</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>token_likelihoods</td>\n",
       "      <td>PREDICTED_VALUE</td>\n",
       "      <td>TOKEN_LIKELIHOODS</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>output_text</td>\n",
       "      <td>PREDICTED_VALUE</td>\n",
       "      <td>UNSTRUCTURED_TEXT</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>finish_reason</td>\n",
       "      <td>NON_INPUT_DATA</td>\n",
       "      <td>STRING</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[{value: length}, {value: stop}]</td>\n",
       "      <td>None</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>readability</td>\n",
       "      <td>NON_INPUT_DATA</td>\n",
       "      <td>FLOAT</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[0, 100]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>correct</td>\n",
       "      <td>NON_INPUT_DATA</td>\n",
       "      <td>BOOLEAN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[{value: True}, {value: False}]</td>\n",
       "      <td>None</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>user_feedback</td>\n",
       "      <td>NON_INPUT_DATA</td>\n",
       "      <td>STRING</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>user_feedback_score</td>\n",
       "      <td>NON_INPUT_DATA</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[{value: -1}, {value: 0}, {value: 1}, {value: ...</td>\n",
       "      <td>None</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name            stage         value_type categorical  \\\n",
       "0           user_input   PIPELINE_INPUT  UNSTRUCTURED_TEXT        True   \n",
       "1    token_likelihoods  PREDICTED_VALUE  TOKEN_LIKELIHOODS       False   \n",
       "2          output_text  PREDICTED_VALUE  UNSTRUCTURED_TEXT        True   \n",
       "3        finish_reason   NON_INPUT_DATA             STRING        True   \n",
       "4          readability   NON_INPUT_DATA              FLOAT       False   \n",
       "5              correct   NON_INPUT_DATA            BOOLEAN        True   \n",
       "6        user_feedback   NON_INPUT_DATA             STRING        True   \n",
       "7  user_feedback_score   NON_INPUT_DATA            INTEGER        True   \n",
       "\n",
       "  is_unique                                         categories  bins  \\\n",
       "0      True                                                 []  None   \n",
       "1     False                                                 []  None   \n",
       "2     False                                                 []  None   \n",
       "3     False                   [{value: length}, {value: stop}]  None   \n",
       "4     False                                                 []  None   \n",
       "5     False                    [{value: True}, {value: False}]  None   \n",
       "6      True                                                 []  None   \n",
       "7     False  [{value: -1}, {value: 0}, {value: 1}, {value: ...  None   \n",
       "\n",
       "          range monitor_for_bias  \n",
       "0  [None, None]            False  \n",
       "1  [None, None]            False  \n",
       "2  [None, None]            False  \n",
       "3  [None, None]            False  \n",
       "4      [0, 100]            False  \n",
       "5  [None, None]            False  \n",
       "6  [None, None]            False  \n",
       "7  [None, None]            False  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# register arthur model type\n",
    "arthur_model_gpt3 = arthur.model(\n",
    "    display_name=\"OpenAI_GPT3\",\n",
    "    input_type=InputType.NLP, \n",
    "    output_type=OutputType.TokenSequence)\n",
    "\n",
    "# register attributes for token sequence model\n",
    "arthur_model_gpt3.build_token_sequence_model(\n",
    "    input_column=\"user_input\", \n",
    "    output_text_column=\"output_text\",\n",
    "    output_likelihood_column=\"token_likelihoods\"\n",
    ")\n",
    "\n",
    "# register additional non-input attributes\n",
    "arthur_model_gpt3.add_attribute(\n",
    "    name='finish_reason', \n",
    "    stage=Stage.NonInputData,\n",
    "    value_type=ValueType.String,\n",
    "    categorical=True,\n",
    "    categories=['length', 'stop']\n",
    ")\n",
    "\n",
    "arthur_model_gpt3.add_attribute(\n",
    "    name=\"readability\",\n",
    "    stage=Stage.NonInputData,\n",
    "    value_type=ValueType.Float,\n",
    "    min_range=0,\n",
    "    max_range=100\n",
    ")\n",
    "\n",
    "arthur_model_gpt3.add_attribute(\n",
    "    name=\"correct\",\n",
    "    stage=Stage.NonInputData,\n",
    "    value_type=ValueType.Boolean,\n",
    "    categorical=True,\n",
    "    categories=[True,False]\n",
    ")\n",
    "\n",
    "arthur_model_gpt3.add_attribute(\n",
    "    name=\"user_feedback\",\n",
    "    stage=Stage.NonInputData,\n",
    "    value_type=ValueType.String,\n",
    "    categorical=True,\n",
    "    is_unique=True\n",
    ")\n",
    "\n",
    "arthur_model_gpt3.add_attribute(\n",
    "    name=\"user_feedback_score\",\n",
    "    stage=Stage.NonInputData,\n",
    "    value_type=ValueType.Integer,\n",
    "    categorical=True,\n",
    "    categories=[-1,0,1,2,3,4,5,6,7,8,9,10]\n",
    ")\n",
    "\n",
    "arthur_model_gpt3.review()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0290514d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f0ec2b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Before saving, be sure to review your model to make sure everything is correct. We already saw the model schema returned by `ArthurModel.build()`, but we have since changed our attribute congiruations. Therefore we call `ArthurModel.review()` to see that our changed attributes look correct before saving to the platform. See the [onboarding walkthrough on the Arthur docs](https://docs.arthur.ai/user-guide/walkthroughs/model-onboarding/index.html#review-model) for tips on reviewing your model.\n",
    "\n",
    "Note that while we capture the ranges of the attributes in this schema, they don’t need to be exact and won’t affect any performance calculations. They’re used as metadata to configure plots in the online Arthur dashboard, but never affect data drift or any other computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f77b552b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>stage</th>\n",
       "      <th>value_type</th>\n",
       "      <th>categorical</th>\n",
       "      <th>is_unique</th>\n",
       "      <th>categories</th>\n",
       "      <th>bins</th>\n",
       "      <th>range</th>\n",
       "      <th>monitor_for_bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_input</td>\n",
       "      <td>PIPELINE_INPUT</td>\n",
       "      <td>UNSTRUCTURED_TEXT</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>token_likelihoods</td>\n",
       "      <td>PREDICTED_VALUE</td>\n",
       "      <td>TOKEN_LIKELIHOODS</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>output_text</td>\n",
       "      <td>PREDICTED_VALUE</td>\n",
       "      <td>UNSTRUCTURED_TEXT</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>finish_reason</td>\n",
       "      <td>NON_INPUT_DATA</td>\n",
       "      <td>STRING</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[{value: length}, {value: stop}]</td>\n",
       "      <td>None</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>readability</td>\n",
       "      <td>NON_INPUT_DATA</td>\n",
       "      <td>FLOAT</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[0, 100]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>correct</td>\n",
       "      <td>NON_INPUT_DATA</td>\n",
       "      <td>BOOLEAN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[{value: True}, {value: False}]</td>\n",
       "      <td>None</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>user_feedback</td>\n",
       "      <td>NON_INPUT_DATA</td>\n",
       "      <td>STRING</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>user_feedback_score</td>\n",
       "      <td>NON_INPUT_DATA</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[{value: -1}, {value: 0}, {value: 1}, {value: ...</td>\n",
       "      <td>None</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name            stage         value_type categorical  \\\n",
       "0           user_input   PIPELINE_INPUT  UNSTRUCTURED_TEXT        True   \n",
       "1    token_likelihoods  PREDICTED_VALUE  TOKEN_LIKELIHOODS       False   \n",
       "2          output_text  PREDICTED_VALUE  UNSTRUCTURED_TEXT        True   \n",
       "3        finish_reason   NON_INPUT_DATA             STRING        True   \n",
       "4          readability   NON_INPUT_DATA              FLOAT       False   \n",
       "5              correct   NON_INPUT_DATA            BOOLEAN        True   \n",
       "6        user_feedback   NON_INPUT_DATA             STRING        True   \n",
       "7  user_feedback_score   NON_INPUT_DATA            INTEGER        True   \n",
       "\n",
       "  is_unique                                         categories  bins  \\\n",
       "0      True                                                 []  None   \n",
       "1     False                                                 []  None   \n",
       "2     False                                                 []  None   \n",
       "3     False                   [{value: length}, {value: stop}]  None   \n",
       "4     False                                                 []  None   \n",
       "5     False                    [{value: True}, {value: False}]  None   \n",
       "6      True                                                 []  None   \n",
       "7     False  [{value: -1}, {value: 0}, {value: 1}, {value: ...  None   \n",
       "\n",
       "          range monitor_for_bias  \n",
       "0  [None, None]            False  \n",
       "1  [None, None]            False  \n",
       "2  [None, None]            False  \n",
       "3  [None, None]            False  \n",
       "4      [0, 100]            False  \n",
       "5  [None, None]            False  \n",
       "6  [None, None]            False  \n",
       "7  [None, None]            False  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review the model attribute properties in the model schema\n",
    "arthur_model_gpt3.review()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b295dc3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we save the model. \n",
    "\n",
    "Note that this will be the first call to send data to the Arthur platform so far in this example - no information has been sent yet to the platform.\n",
    "\n",
    "The method `ArthurModel.save()` sends an API request to Arthur to validate your model - if there are any problems with your model schema, this method will result in an error informing you how to correct your model's configuration. If no errors are found, the model will be saved to the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e956292f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:46:22 - arthurai - We have registered the  model with Arthur and are getting it ready to accept inferences...\n",
      "We are still working on getting your model ready to accept inferences...\n",
      "10:47:38 - arthurai - Model Creation Completed successfully, you can now send Data to Arthur.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bc18efc2-4ead-4cb8-9e9f-df2b3b2ca73f'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate the model and save it onto the Arthur platform\n",
    "arthur_model_gpt3.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "beadac18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:47:40 - arthurai - Starting upload (0.006 MB in 1 files), depending on data size this may take a few minutes\n",
      "10:47:40 - arthurai - Upload completed: /var/folders/8v/8v36mrp907z7lp5d4cd7yf4h0000gn/T/tmpzndw1ied/bc18efc2-4ead-4cb8-9e9f-df2b3b2ca73f-0.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'counts': {'success': 5, 'failure': 0, 'total': 5}, 'failures': [[]]},\n",
       " {'dataset_close_result': {'message': 'success'}})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arthur_model_gpt3.set_reference_data(data=gpt3_ref_data.drop(['token_likelihoods'], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7464bea3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id='sending_inferences_content'></a>\n",
    "\n",
    "## Sending Inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e669b6d2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Using `ArthurModel.send_inferences()`\n",
    "\n",
    "We send our inference data to the platform along with some unique IDs. Timestamps will be auto-generated for these inferences by the `send_inferences` function from the Arthur SDK. See [our API docs for sending inferences](https://docs.arthur.ai/api-documentation/v3-api-docs.html#tag/inferences/paths/~1models~1%7Bmodel_id%7D~1inferences/post) for the full specification of inference ingestion in Arthur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bf4fa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First and last timestamps: 2023-04-18 to 2023-04-21\n"
     ]
    }
   ],
   "source": [
    "from arthurai.util import generate_timestamps\n",
    "# generate fake timestamps for each inference over the last month\n",
    "inference_timestamps = list(generate_timestamps(len(gpt3_inf_data), '3d', 'now', 'D'))\n",
    "\n",
    "print(f\"First and last timestamps: {inference_timestamps[0].strftime('%Y-%m-%d')} to {inference_timestamps[-1].strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bc2763e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of successful and # of failing inference uploads: {'failure': 0, 'success': 5, 'total': 5}\n"
     ]
    }
   ],
   "source": [
    "inference_result = arthur_model_gpt3.send_inferences(\n",
    "    gpt3_inf_data, \n",
    "    inference_timestamps=inference_timestamps,\n",
    "    partner_inference_ids=[shortuuid.uuid() for _ in range(len(gpt3_inputs_inf))])\n",
    "print('# of successful and # of failing inference uploads:', inference_result['counts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd79eccc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## See Model in Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9899c692",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br> <a style=\"font-size:200%\" href=https://dev.arthur.ai/model/None/overview>See your model (OpenAI_GPT3) in the Arthur Dashboard</a> <br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the code below will render a link for you to view your model in the Arthur Dashboard\n",
    "\n",
    "def render_arthur_model_dashboard_link(arthur, arthur_model):\n",
    "    url = 'https://' + ''.join(arthur.client.api_base_url.split('/')[1:-2])\n",
    "    link_text = f\"See your model ({arthur_model.display_name}) in the Arthur Dashboard\"\n",
    "    href_string = f\"{url}/model/{arthur_model.id}/overview\"\n",
    "    html_string = f'<br> <a style=\"font-size:200%\" href={href_string}>{link_text}</a> <br>'\n",
    "    display(HTML(html_string))\n",
    "\n",
    "render_arthur_model_dashboard_link(arthur, arthur_model_gpt3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cb3132",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once your inference data has been uploaded to the platform, you can see your model by following the above link to the model dashboard page to see an overview of the model and browse its inference data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096714d1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fd9f85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
