{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:26:24.864825Z",
     "start_time": "2021-09-02T19:26:24.660444Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:26:28.899396Z",
     "start_time": "2021-09-02T19:26:24.867028Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('app').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SparkML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:26:32.864825Z",
     "start_time": "2021-09-02T19:26:28.901704Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = (spark.read.csv('./data/boston_housing.csv', header=True, inferSchema=True)\n",
    "        .withColumnRenamed(\"medv\", \"medv_gt\"))\n",
    "data.printSchema()\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:26:32.910152Z",
     "start_time": "2021-09-02T19:26:32.867710Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:26:32.969751Z",
     "start_time": "2021-09-02T19:26:32.912712Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_columns = data.columns[:-1] # here we omit the final column\n",
    "assembler = VectorAssembler(inputCols=feature_columns,outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:26:33.031867Z",
     "start_time": "2021-09-02T19:26:32.971434Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "algo = LinearRegression(featuresCol=\"features\", labelCol=\"medv_gt\", predictionCol=\"medv_pred\",\n",
    "                        maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Save Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:26:35.078399Z",
     "start_time": "2021-09-02T19:26:33.034799Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[assembler, algo]) \n",
    "\n",
    "fitted_pipeline = pipeline.fit(train)\n",
    "\n",
    "fitted_pipeline.transform(test).select('medv_pred').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:26:36.160842Z",
     "start_time": "2021-09-02T19:26:35.080610Z"
    }
   },
   "outputs": [],
   "source": [
    "fitted_pipeline.write().overwrite().save('./data/models/boston_housing_spark_model_pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:26:37.771199Z",
     "start_time": "2021-09-02T19:26:36.162313Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "loaded_model_pipeline = PipelineModel.load(\"./data/models/boston_housing_spark_model_pipeline\")\n",
    "\n",
    "predicted_train_data = loaded_model_pipeline.transform(test).drop(\"features\")\n",
    "\n",
    "predicted_test_data = loaded_model_pipeline.transform(test).drop(\"features\")\n",
    "predicted_test_data.select('medv_pred').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-board to Arthur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:26:39.226697Z",
     "start_time": "2021-09-02T19:26:37.772863Z"
    }
   },
   "outputs": [],
   "source": [
    "from arthurai import ArthurAI\n",
    "from arthurai.common.constants import InputType, OutputType, Stage, ValueType\n",
    "from numpy.random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:26:39.425322Z",
     "start_time": "2021-09-02T19:26:39.228434Z"
    }
   },
   "outputs": [],
   "source": [
    "# credentials are being passed to the client via environment variables ARTHUR_API_KEY & ARTHUR_ENDPOINT_URL\n",
    "connection = ArthurAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:26:39.429384Z",
     "start_time": "2021-09-02T19:26:39.426786Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the model with overall metadata about its input and output types\n",
    "MODEL_METADATA = {\n",
    "    \"partner_model_id\": f\"SparkBostonHousingModel_FG-{datetime.now().strftime('%Y%m%d%H%M%S')}\",\n",
    "    \"description\": \"Spark Boston Housing Model\",\n",
    "    \"input_type\": InputType.Tabular,\n",
    "    \"model_type\": OutputType.Regression,\n",
    "    \"tags\": ['Spark'],\n",
    "    \"is_batch\": True\n",
    "}\n",
    "\n",
    "model = connection.model(**MODEL_METADATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:26:39.798397Z",
     "start_time": "2021-09-02T19:26:39.432434Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_to_ground_truth_map = {\"medv_pred\": \"medv_gt\"}\n",
    "model.build(predicted_train_data.toPandas(), pred_to_ground_truth_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:26:57.293598Z",
     "start_time": "2021-09-02T19:26:57.289918Z"
    }
   },
   "outputs": [],
   "source": [
    "# chas and rad are categorical, check the inferred possible categories\n",
    "print(model.get_attribute('chas').categories)\n",
    "print(model.get_attribute('rad').categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T16:40:46.296378Z",
     "start_time": "2021-08-20T16:40:43.434551Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_id = model.save()\n",
    "\n",
    "with open(\"fullguide_model_id.txt\", \"w\") as f:\n",
    "    f.write(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can fetch a model by ID. for example pull the last-created model:\n",
    "# with open(\"fullguide_model_id.txt\", \"r\") as f:\n",
    "#     model_id = f.read()\n",
    "# arthur_model = connection.get_model(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enabling Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T16:51:22.171037Z",
     "start_time": "2021-08-20T16:51:20.154216Z"
    }
   },
   "outputs": [],
   "source": [
    "# When using a spark model be sure to allocate at least 2 cpus to the model server.\n",
    "# This can scale as you change the configurations of the spark session in your entrypoint\n",
    "# script.\n",
    "import os\n",
    "\n",
    "model.enable_explainability(df=predicted_train_data.toPandas(), project_directory=os.path.abspath(\"\"),\n",
    "                            user_predict_function_import_path='entrypoint',\n",
    "                            streaming_explainability_enabled=False,\n",
    "                            requirements_file='requirements.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send an inference batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T17:12:30.915724Z",
     "start_time": "2021-08-20T17:12:30.675254Z"
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# In order to send ground truth we must use an external id to match up rows in the ground truth dataframe and\n",
    "# inferences dataframe\n",
    "predicted_test_data = (predicted_test_data.withColumn('inference_timestamp', F.current_timestamp())\n",
    "                                          .withColumn(\"partner_inference_id\", F.rand().cast(StringType())))\n",
    "predicted_test_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T17:12:32.984893Z",
     "start_time": "2021-08-20T17:12:32.939573Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now we separate out the inference input dataframe frame and the ground truth dataframe\n",
    "pipeline_input_attr_names = [attr.name for attr in model.get_attributes(Stage.ModelPipelineInput)]\n",
    "columns_to_select = pipeline_input_attr_names + ['medv_pred', 'partner_inference_id', 'inference_timestamp']\n",
    "batch_inferences = predicted_test_data.select(columns_to_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T17:12:54.719765Z",
     "start_time": "2021-08-20T17:12:54.698872Z"
    }
   },
   "outputs": [],
   "source": [
    "# getting ground truth batch dataframe\n",
    "columns_to_select = ['medv_gt', 'partner_inference_id']\n",
    "ground_truth_batch = predicted_test_data.select(columns_to_select).withColumn('ground_truth_timestamp', F.current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T17:12:55.961918Z",
     "start_time": "2021-08-20T17:12:55.295797Z"
    }
   },
   "outputs": [],
   "source": [
    "# write inferences dataframe to parquet file\n",
    "batch_inferences.write.mode('overwrite').parquet(\"./data/batch_inference_files/batch_inferences.parquet\")\n",
    "ground_truth_batch.write.mode('overwrite').parquet(\"./data/batch_ground_truth_files/ground_truth.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T17:12:59.779116Z",
     "start_time": "2021-08-20T17:12:59.254220Z"
    }
   },
   "outputs": [],
   "source": [
    "model.send_bulk_inferences(directory_path='./data/batch_inference_files/', batch_id=\"batch1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T17:13:01.420358Z",
     "start_time": "2021-08-20T17:13:01.232722Z"
    }
   },
   "outputs": [],
   "source": [
    "model.send_bulk_ground_truths(directory_path='./data/batch_ground_truth_files/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
