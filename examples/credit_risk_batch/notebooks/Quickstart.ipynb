{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:08:43.873764Z",
     "start_time": "2021-09-02T19:08:42.326020Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from arthurai import ArthurAI\n",
    "from arthurai.common.constants import InputType, OutputType, Stage\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:08:43.879433Z",
     "start_time": "2021-09-02T19:08:43.875640Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from model_utils import load_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this guide, we'll use the credit dataset (and a pre-trained model) to onboard a new model to the Arthur platform. We'll walk through registering the model using a sample of the training data. This is an example of a batch model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up connection\n",
    "Supply your API Key below to autheticate with the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:08:44.027381Z",
     "start_time": "2021-09-02T19:08:43.881347Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# credentials are being passed to the client via environment variables\n",
    "connection = ArthurAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll instantiate a model object with a small amount of metadata about the model input and output types. Then, we'll use a sample of the training data to register the full data schema for this Tabular model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:08:45.009266Z",
     "start_time": "2021-09-02T19:08:45.006907Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arthur_model = connection.model(partner_model_id=f\"CreditRisk_Batch_QS-{datetime.now().strftime('%Y%m%d%H%M%S')}\",\n",
    "                                display_name=\"Credit Risk Batch\",\n",
    "                                input_type=InputType.Tabular,\n",
    "                                output_type=OutputType.Multiclass,\n",
    "                                is_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:08:45.243900Z",
     "start_time": "2021-09-02T19:08:45.180027Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = load_datasets(\"../fixtures/datasets/credit_card_default.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:08:45.366758Z",
     "start_time": "2021-09-02T19:08:45.361965Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:08:45.528446Z",
     "start_time": "2021-09-02T19:08:45.514842Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:08:49.217032Z",
     "start_time": "2021-09-02T19:08:47.840755Z"
    }
   },
   "outputs": [],
   "source": [
    "# load our pre-trained classifier so we can generate predictions\n",
    "sk_model = joblib.load(\"../fixtures/serialized_models/credit_model.pkl\")\n",
    "\n",
    "# get model predictions\n",
    "preds = sk_model.predict_proba(X_train)\n",
    "X_train[\"prediction_1\"] = preds[:, 1]\n",
    "\n",
    "# # get ground truth labels\n",
    "X_train[\"gt\"] = Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to register the schema for the outputs of the model: what will a typical prediction look like and what will a typical ground truth look like? What names, shapes, and datatypes should Arthur expect for these objects?\n",
    "\n",
    "We'll do this all in one step with the *.build()* method. All we need to supply is:\n",
    "  * the training dataframe\n",
    "  * the mapping that related predictions to ground truth\n",
    "  * positive predicted attribute label\n",
    "  \n",
    "Our classifier will be making predictions about class *0* and class *1* and will return a probability score for each class. Therefore, we'll set up a name *prediction_0* and a name *prediction_1*. Additionally, our groundtruth will be either a 0 or 1, but we'll always represent ground truth in the one-hot-endoded form. Therefore, we create two fields called *gt_0* and *gt_1*. We link these all up in a dictionary and pass that to the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:08:53.912810Z",
     "start_time": "2021-09-02T19:08:53.752001Z"
    }
   },
   "outputs": [],
   "source": [
    "# Map our prediction attribute to the ground truth value\n",
    "prediction_to_ground_truth_map = {\n",
    "    \"prediction_1\": 1\n",
    "}\n",
    "\n",
    "arthur_model.build(X_train, \n",
    "                   ground_truth_column=\"gt\",\n",
    "                   pred_to_ground_truth_map=prediction_to_ground_truth_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before saving, you can also review your model to make sure everything is correct from the output of `arthur_model.build()` or via `arthur_model.review()`.\n",
    "\n",
    "When saving your model, the data is saved as the reference set, which is used as the baseline data for tracking data drift. Often, this is the training data for the associated model. Our reference dataset should include:\n",
    "  * inputs \n",
    "  * ground truth\n",
    "  * model predictions\n",
    "  \n",
    "This way, Arthur can monitor for drift and stability in all of these aspects. \n",
    "\n",
    "If you've already created your model, you can fetch it from the Arthur API. Retrieve a Model ID from the output of the `arthur_model.save()` call below, or the URL of your model page in the Arthur Dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_id = arthur_model.save()\n",
    "with open(\"quickstart_model_id.txt\", \"w\") as f:\n",
    "    f.write(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can fetch a model by ID. for example pull the last-created model:\n",
    "# with open(\"quickstart_model_id.txt\", \"r\") as f:\n",
    "#     model_id = f.read()\n",
    "# arthur_model = connection.get_model(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending Batches of Inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test data and trained model. Let's familiarize ourselves with the data and the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions/scores from your model should match the column names in the registered schema. If we take a look above at *arthur_model.review()* we'll recall that columns we created correspond to the clasiffier's output probabilities over the classes (\"prediction_1\" and \"prediction_0\") and the corresponding ground truth over the possible clases in one-hot form (\"gt_1\" and \"gt_0\").\n",
    "\n",
    "Aside from these model-specific columns, there are two standard inputs which are needed to indentify inferences.\n",
    "* First, each inference needs a unique identifier so that it can later be joined with ground truth. Include a column named **partner_inference_id** and ensure these IDs are unique across batches. For example, if you run predictions across your customer base on a daily-batch cadence, then a unique identfier could be composed of your customer_id plus the date.   \n",
    "* Second, each inference needs an **inference_timestamp** and these don't have to be unique.\n",
    "\n",
    "We'll use our clasifier to score a batch of inputs and then assemble those inputs and predictions into a dataframe with the matching column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "num_batches = 20\n",
    "batch_ids = []\n",
    "\n",
    "for i in range(num_batches):\n",
    "    batch_size=np.random.randint(1000, 5000)\n",
    "    batch_id = f\"batch_{str(uuid4()).split('-')[1]}\"\n",
    "    batch_ids.append(batch_id)\n",
    "\n",
    "    # generate a small batch of rows from the test set, create unique id for each row\n",
    "    batch_df = X_test.sample(batch_size)\n",
    "    inference_ids = [f\"{batch_id}-inf_{i}\" for i in batch_df.index]\n",
    "    \n",
    "    # calculate predictions on those rows, fetch ground truth for those rows\n",
    "    batch_predictions = sk_model.predict_proba(batch_df)\n",
    "    batch_ground_truths = Y_test[batch_df.index]\n",
    "    \n",
    "    # need to include model prediction columns, and partner_inference_id\n",
    "    batch_df[\"prediction_1\"] = batch_predictions[:, 1]\n",
    "    \n",
    "    # assemble the inference-wise groundtruth and upload\n",
    "    ground_truth_df = pd.DataFrame({'gt': batch_ground_truths})\n",
    "\n",
    "    arthur_model.send_inferences(batch_df, batch_id=batch_id, partner_inference_ids=inference_ids)\n",
    "    arthur_model.update_inference_ground_truths(ground_truth_df, partner_inference_ids=inference_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realistically, there will be some delay before you have ground truth for your model's predictions. Whether that ground truth is accessible after one minute or one year, the *update_inference_ground_truths()* method can be called at any later time. The ground truth (labels) will joined with their corresponding predictions to yield accuracy measures. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d485cb42aaa05640787d51ff6b98576e6591c8fe1b0e087ca1599e137e7b48f2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
