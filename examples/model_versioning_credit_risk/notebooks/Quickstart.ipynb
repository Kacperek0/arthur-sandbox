{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:07:37.572976Z",
     "start_time": "2021-09-02T19:07:35.176268Z"
    },
    "scrolled": true
   },
   "source": [
    "<h1 align=\"center\">Model Versioning Example Notebook</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this guide, we will demonstrate how to use model versioning in the Arthur platform. We'll use the credit dataset (and a pre-trained model) to onboard 3 new models to the Arthur platform and put them together in the same Model Group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arthurai import ArthurAI\n",
    "from arthurai.common.constants import InputType, OutputType, Stage\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:07:37.579427Z",
     "start_time": "2021-09-02T19:07:37.575012Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from model_utils import load_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up connection\n",
    "Supply your API Key below to authenticate with the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:07:38.398242Z",
     "start_time": "2021-09-02T19:07:38.181262Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# connect to Arthur\n",
    "# UNCOMMENT the two lines below and enter your details\n",
    "arthur = ArthurAI(\n",
    "    # url=\"https://app.arthur.ai\",  # you can also pass this through the ARTHUR_ENDPOINT_URL environment variable\n",
    "    # login=\"<YOUR_USERNAME_OR_EMAIL>\",  # you can also pass this through the ARTHUR_LOGIN environment variable\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model v1 (Logistic Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the first model in a group requires no extra effort. A model group is created automatically when you create the model. Let's start with a logistic regression model trained on credit card data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:07:45.116551Z",
     "start_time": "2021-09-02T19:07:43.814175Z"
    }
   },
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = load_datasets(\"../fixtures/datasets/credit_card_default.csv\")\n",
    "\n",
    "# load our pre-trained classifier so we can generate predictions\n",
    "sk_model = joblib.load(\"../fixtures/serialized_models/credit_lr.pkl\")  # Logistic Regression pickle file\n",
    "\n",
    "# get model predictions\n",
    "preds = sk_model.predict_proba(X_train)\n",
    "X_train[\"prediction_1\"] = preds[:, 1]\n",
    "X_train[\"prediction_0\"] = preds[:, 0]\n",
    "\n",
    "# get ground truth labels\n",
    "X_train[\"gt_1\"] = Y_train\n",
    "X_train[\"gt_0\"] = 1-Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T21:13:21.427158Z",
     "start_time": "2021-08-02T21:13:21.423669Z"
    }
   },
   "source": [
    "We'll instantiate a model object with a small amount of metadata about the model input and output types. Then, we'll extract the schema from the training data to build the complete model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:07:49.617410Z",
     "start_time": "2021-09-02T19:07:49.613680Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arthur_model_log_reg = arthur.model(partner_model_id=f\"CreditRiskModel_QS_logistic_regression_{datetime.now().strftime('%Y%m%d%H%M%S')}_v1\",\n",
    "                                        display_name=\"Credit Risk\",\n",
    "                                        input_type=InputType.Tabular,\n",
    "                                        output_type=OutputType.Multiclass)\n",
    "\n",
    "prediction_to_ground_truth_map = {\n",
    "    \"prediction_0\": \"gt_0\",\n",
    "    \"prediction_1\": \"gt_1\"\n",
    "}\n",
    "\n",
    "arthur_model_log_reg.build(X_train,\n",
    "                           pred_to_ground_truth_map=prediction_to_ground_truth_map,\n",
    "                           positive_predicted_attr=\"prediction_1\",\n",
    "                           non_input_columns=['SEX'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know we'll want to make multiple versions of this model, let's give this version a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arthur_model_log_reg.version_label = \"logistic_regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although a label is not neccessary, it can be helpful when you need to distinguish between several versions. Regardless of whether you give a version label or not, you will always be able to distinguish versions by an automatically assigned, incrementing `version_sequence_num`. The sequence number starts at 1 and each version in the group is assigned the next natural number, e.g. 1, 2, 3, and so on.\n",
    "\n",
    "Although the model has been all but saved to Arthur, you'll notice that there is still no Model Group ID associated with this model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arthur_model_log_reg.model_group_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't have a model group we want to assign this model to we can leave it as `None`. A model group will automatically be created with it when you call save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_id = arthur_model_log_reg.save()\n",
    "log_reg_model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has been saved, we have access to the information about model_group associated with it.  We retrieve it by passing in the model into the `arthur.get_model_group()` function.  Alternatively, we could pass in the model_group_id directly with `arthur.get_model_group(model_group_id)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_group = arthur.get_model_group(arthur_model_log_reg)\n",
    "model_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending Inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some predictions with our model and send them to Arthur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T21:17:40.195457Z",
     "start_time": "2021-08-02T21:17:40.192922Z"
    }
   },
   "outputs": [],
   "source": [
    "from arthurai.core.decorators import log_prediction\n",
    "\n",
    "@log_prediction(arthur_model_log_reg)\n",
    "def model_predict(input_vec):\n",
    " return sk_model.predict_proba(input_vec)[0]\n",
    "\n",
    "# 10 timestamps over the last week\n",
    "timestamps = pd.date_range(start=datetime.now(pytz.utc) - timedelta(days=7),\n",
    "                           end=datetime.now(pytz.utc),\n",
    "                           periods=10)\n",
    "\n",
    "inference_ids = {}\n",
    "for timestamp in timestamps:\n",
    "    for i in range(np.random.randint(50, 100)):\n",
    "        datarecord = X_test.sample(1)  # fetch a random row\n",
    "        prediction, inference_id = model_predict(datarecord, inference_timestamp=timestamp)  # predict and log\n",
    "        inference_ids[inference_id] = datarecord.index[0]  # record the inference ID with the Pandas index\n",
    "    print(f\"Logged {i+1} inferences with Arthur from {timestamp.strftime('%m/%d')}\")\n",
    "\n",
    "gt_df = pd.DataFrame({'partner_inference_id': inference_ids.keys(),\n",
    "                      'gt_1': Y_test[inference_ids.values()],\n",
    "                      'gt_0': 1 - Y_test[inference_ids.values()]})\n",
    "_ = arthur_model_log_reg.update_inference_ground_truths(gt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model v2 (Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you decide you don't like the logistic regression model and you decide you want to use your new random forest model. We want to link this new implementation as a new version of the first model (i.e. put it inside the same model group), so let's do that.\n",
    "\n",
    "To start, create the new model, just like before.  Remember, you will need to send new inferences, reference data, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:08:45.009266Z",
     "start_time": "2021-09-02T19:08:45.006907Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = load_datasets(\"../fixtures/datasets/credit_card_default.csv\")\n",
    "\n",
    "# load our pre-trained classifier so we can generate predictions\n",
    "sk_model = joblib.load(\"../fixtures/serialized_models/credit_rf.pkl\")  # Random Forest pickle file\n",
    "\n",
    "# get model predictions\n",
    "preds = sk_model.predict_proba(X_train)\n",
    "X_train[\"prediction_1\"] = preds[:, 1]\n",
    "X_train[\"prediction_0\"] = preds[:, 0]\n",
    "\n",
    "# get ground truth labels\n",
    "X_train[\"gt_1\"] = Y_train\n",
    "X_train[\"gt_0\"] = 1-Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, let's build the Arthur Model wrapper and give it some updated information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:08:45.243900Z",
     "start_time": "2021-09-02T19:08:45.180027Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arthur_model_rand_forest = arthur.model(partner_model_id=f\"CreditRiskModel_QS_random_forest_{datetime.now().strftime('%Y%m%d%H%M%S')}_v2\",\n",
    "                                            display_name=\"Credit Risk v2 (Random Forest)\",\n",
    "                                            input_type=InputType.Tabular,\n",
    "                                            output_type=OutputType.Multiclass)\n",
    "\n",
    "prediction_to_ground_truth_map = {\n",
    "    \"prediction_0\": \"gt_0\",\n",
    "    \"prediction_1\": \"gt_1\"\n",
    "}\n",
    "\n",
    "arthur_model_rand_forest.build(X_train,\n",
    "                               pred_to_ground_truth_map=prediction_to_ground_truth_map,\n",
    "                               positive_predicted_attr=\"prediction_1\",\n",
    "                               non_input_columns=['SEX'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the new model to be a new version of the first model\n",
    "\n",
    "Since we want to associate this new model with our previous model, we want to simply set the model_group_id on the model _before_ we call `.save()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arthur_model_rand_forest.model_group_id = model_group.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you could call `add_version` on `model_group` like so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_group.add_version(arthur_model_rand_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we save the model, the Arthur platform will add this new model to the same model group as the first model and assign it a sequence number of 2. Of course, to help us distinguish more clearly between versions 1 and 2, let's give this the label, \"random_forest\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arthur_model_rand_forest.version_label = \"random_forest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rand_forest_model_id = arthur_model_rand_forest.save()\n",
    "rand_forest_model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending Inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's send some inferences with this new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arthurai.core.decorators import log_prediction\n",
    "\n",
    "@log_prediction(arthur_model_rand_forest)\n",
    "def model_predict(input_vec):\n",
    " return sk_model.predict_proba(input_vec)[0]\n",
    "\n",
    "# 10 timestamps over the last week\n",
    "timestamps = pd.date_range(start=datetime.now(pytz.utc) - timedelta(days=7),\n",
    "                           end=datetime.now(pytz.utc),\n",
    "                           periods=10)\n",
    "\n",
    "inference_ids = {}\n",
    "for timestamp in timestamps:\n",
    "    for i in range(np.random.randint(50, 100)):\n",
    "        datarecord = X_test.sample(1)  # fetch a random row\n",
    "        prediction, inference_id = model_predict(datarecord, inference_timestamp=timestamp)  # predict and log\n",
    "        inference_ids[inference_id] = datarecord.index[0]  # record the inference ID with the Pandas index\n",
    "    print(f\"Logged {i+1} inferences with Arthur from {timestamp.strftime('%m/%d')}\")\n",
    "\n",
    "gt_df = pd.DataFrame({'partner_inference_id': inference_ids.keys(),\n",
    "                      'gt_1': Y_test[inference_ids.values()],\n",
    "                      'gt_0': 1 - Y_test[inference_ids.values()]})\n",
    "_ = arthur_model_rand_forest.update_inference_ground_truths(gt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model v3 (\"Fair\" Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, you want to iterate on your model because you feel like the model is biased.  Let's onboard a model v3 that is more fair...or at least we hope it is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:08:45.009266Z",
     "start_time": "2021-09-02T19:08:45.006907Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = load_datasets(\"../fixtures/datasets/credit_card_default.csv\")\n",
    "\n",
    "# load our pre-trained classifier so we can generate predictions\n",
    "sk_model = joblib.load(\"../fixtures/serialized_models/credit_frf.pkl\")  # \"Fair\" Random Forest pickle file\n",
    "\n",
    "# get model predictions\n",
    "preds = sk_model.predict_proba(X_train)\n",
    "X_train[\"prediction_1\"] = preds[:, 1]\n",
    "X_train[\"prediction_0\"] = preds[:, 0]\n",
    "\n",
    "# get ground truth labels\n",
    "X_train[\"gt_1\"] = Y_train\n",
    "X_train[\"gt_0\"] = 1-Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as before, set the model info and build it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T19:08:45.243900Z",
     "start_time": "2021-09-02T19:08:45.180027Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arthur_model_fair_rf = arthur.model(partner_model_id=f\"CreditRiskModel_QS_fair_random_forest_{datetime.now().strftime('%Y%m%d%H%M%S')}_v3\",\n",
    "                                        display_name=\"Credit Risk v3 (Fair Random Forest)\",\n",
    "                                        input_type=InputType.Tabular,\n",
    "                                        output_type=OutputType.Multiclass)\n",
    "\n",
    "prediction_to_ground_truth_map = {\n",
    "    \"prediction_0\": \"gt_0\",\n",
    "    \"prediction_1\": \"gt_1\"\n",
    "}\n",
    "\n",
    "arthur_model_fair_rf.build(X_train, pred_to_ground_truth_map=prediction_to_ground_truth_map, positive_predicted_attr=\"prediction_1\", non_input_columns=['SEX'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the new model to be a part of the same model group\n",
    "\n",
    "Again, let's set the model group. Last time we did it by setting the `model_group_id` directly. This time, let's use the `add_version` method just for funsies. We can also set the `version_label` here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_group.add_version(arthur_model_fair_rf, label=\"fair_random_forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternatively..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arthur_model_fair_rf.version_label = \"fair_random_forest\"\n",
    "# arthur_model_fair_rf.model_group_id = model_group.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and finally, let's save this one as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fair_rf_model_id = arthur_model_fair_rf.save()\n",
    "fair_rf_model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending Inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we should send some more inferences too right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arthurai.core.decorators import log_prediction\n",
    "\n",
    "@log_prediction(arthur_model_fair_rf)\n",
    "def model_predict(input_vec):\n",
    " return sk_model.predict_proba(input_vec)[0]\n",
    "\n",
    "# 10 timestamps over the last week\n",
    "timestamps = pd.date_range(start=datetime.now(pytz.utc) - timedelta(days=7),\n",
    "                           end=datetime.now(pytz.utc),\n",
    "                           periods=10)\n",
    "\n",
    "inference_ids = {}\n",
    "for timestamp in timestamps:\n",
    "    for i in range(np.random.randint(50, 100)):\n",
    "        datarecord = X_test.sample(1)  # fetch a random row\n",
    "        prediction, inference_id = model_predict(datarecord, inference_timestamp=timestamp)  # predict and log\n",
    "        inference_ids[inference_id] = datarecord.index[0]  # record the inference ID with the Pandas index\n",
    "    print(f\"Logged {i+1} inferences with Arthur from {timestamp.strftime('%m/%d')}\")\n",
    "\n",
    "gt_df = pd.DataFrame({'partner_inference_id': inference_ids.keys(),\n",
    "                      'gt_1': Y_test[inference_ids.values()],\n",
    "                      'gt_0': 1 - Y_test[inference_ids.values()]})\n",
    "_ = arthur_model_fair_rf.update_inference_ground_truths(gt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can really get into some fun stuff!  I'll put the connection info here just in case you've decided to start in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arthurai import ArthurAI\n",
    "from arthurai.common.constants import InputType, OutputType, Stage\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from model_utils import load_datasets\n",
    "\n",
    "# connect to Arthur\n",
    "# UNCOMMENT the two lines below and enter your details\n",
    "arthur = ArthurAI(\n",
    "    # url=\"https://app.arthur.ai\",  # you can also pass this through the ARTHUR_ENDPOINT_URL environment variable\n",
    "    # login=\"<YOUR_USERNAME_OR_EMAIL>\",  # you can also pass this through the ARTHUR_LOGIN environment variable\n",
    ")\n",
    "\n",
    "# If you want to start by getting the model_group directly from an id, you can use this line of code below\n",
    "# model_group = arthur.get_model_group('<model_group_uuid>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly explore some of the things we can do when playing around with model groups.  You can get properties such as the id..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_group.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_group.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't like that name, you can change it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_group.name = 'Awesome Credit Risk Group'\n",
    "model_group.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same groes for the description as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_group.description = 'Super awesome collection of credit risk models!'\n",
    "model_group.description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those changes will instantly be reflected on the Arthur Platform as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arthur.get_model_group(model_group.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the model group information, we can take a look at a list of versions in the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see the names of the 3 models we created above\n",
    "versions = model_group.get_versions()\n",
    "for version in versions:\n",
    "    print(version.display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to retrieve a specific model_version, then we simply need to specify which version want.\n",
    "\n",
    "We can specify by sequence_num..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_group.get_version(sequence_num=2).display_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can specify by label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_group.get_version(label=\"fair_random_forest\").display_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you really want to get funky, you can start running comparisons between models.  You can get a visualizer by using `model_group.viz()`.  If you want to specify only a subset of versions in the group, you can specify a list of `sequence_nums` or `labels`. as arguments in the `viz()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = model_group.viz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our visualizer set up, lets take a look at a metric series.  Here we see two accuracy metrics shown over time. Throughout this notebook, we sent predictions over a weeklong time horizon. and we can see how these metrics did over time.  The red lines represent Area Under the Curve (AUC) for the models while the blue lines represents False Positive Rate. The lighter the color is, the more recent the model version.\n",
    "\n",
    "Feel free to display other accuracy metrics on the graph by editing the line below. Up to 10 metrics can be supported at one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.metric_series([\"auc\", \"falsePositiveRate\"], time_resolution=\"day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at a drift series.  This graph shows how much drift we're seeing across the 3 models on the \"PAY_0\" attribute. The newest model is shown with the darkest line while the oldest has the lightest line. Since all of these models use the same train/test data, we expect to see these 3 lines overlap quite a bit.\n",
    "\n",
    "Feel free to play around with the drift metric or the variable to show drift on. Like the metric series, up to 10 variables are supported to show data drift!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.drift_series([\"PAY_0\"], drift_metric=\"KLDivergence\", time_resolution=\"hour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Housekeeping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section exists solely to aide automated testing cleanup.  Feel free to ignore if you are a human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = log_reg_model_id  # This is only needed to help with automated model cleanup"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d485cb42aaa05640787d51ff6b98576e6591c8fe1b0e087ca1599e137e7b48f2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
