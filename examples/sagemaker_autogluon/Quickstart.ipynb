{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring an Amazon SageMaker Model with Arthur\n",
    "#### Host a trained machine learning model in Amazon SageMaker and log that model's inferences in Arthur\n",
    "\n",
    "\n",
    "This notebook shows how to:\n",
    "* Host a machine learning model in Amazon SageMaker and capture inference requests, results, and metadata\n",
    "* Set up logging of the inputs and outputs of that model into the Arthur platform\n",
    "\n",
    "\n",
    "**Table of Contents** \n",
    "\n",
    " [Introduction](#intro)\n",
    "1. [Section 1 - Setup](#setup)\n",
    "2. [Section 2 - Deploy pre-trained model with data capture enabled](#deploy)\n",
    "3. [Section 3 - Building your Arthur model.](#BuildArthur)\n",
    "4. [Section 4 - Sending inferences through SageMaker and Capturing the Data](#SendInferences)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Introduction <a id='intro'></a>    \n",
    "\n",
    "Amazon SageMaker provides every developer and data scientist with the ability to build, train, and deploy machine learning (ML) models quickly by bringing together a broad set of capabilities purpose-built for ML. Amazon SageMaker is a fully-managed service that encompasses the entire ML workflow. You can label and prepare your data, choose an algorithm, train a model, and then tune and optimize it for deployment. You can deploy your models to production with Amazon SageMaker. With minimal set up, you can then log that model's inferences in the Arthur platform.  \n",
    "\n",
    "In this notebook, you learn how to use Amazon SageMaker with Arthur to monitor the inferences of your in-production ML models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - Setup <a id='setup'></a>\n",
    "\n",
    "In this section, you will import the necessary libraries, setup variables, and set up access to both Arthur and AWS.\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "* Class definitions specific to the pretrained model we will be using\n",
    "* Your Arthur credentials\n",
    "* Your Arthur model metadata\n",
    "* The AWS region used to host your model.\n",
    "* The IAM role associated with this SageMaker notebook instance.\n",
    "* The S3 bucket used to store the data used to train your model, any additional model data, and the data captured from model invocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime, time, timedelta, timezone\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import boto3\n",
    "from time import sleep\n",
    "from threading import Thread\n",
    "import sys\n",
    "import pytz\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scripts.data import download_model, download_reference_dataset, download_test_dataset, \\\n",
    "    MODEL_METADATA_PATH, REFERENCE_DATA_PATH, TEST_DATA_PATH\n",
    "\n",
    "# Importing sagemaker packages\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role, session, Session, image_uris\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "from sagemaker.processing import ProcessingJob\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.model_monitor import DataCaptureConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class required for our pretrained model\n",
    "\n",
    "from sagemaker.mxnet import MXNetModel\n",
    "from sagemaker import image_uris\n",
    "\n",
    "class AutoGluonInferenceModel(MXNetModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_data,\n",
    "        role,\n",
    "        entry_point,\n",
    "        region,\n",
    "        framework_version,\n",
    "        py_version,\n",
    "        instance_type,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        image_uri = image_uris.retrieve(\n",
    "            \"autogluon\",\n",
    "            region=region,\n",
    "            version=framework_version,\n",
    "            py_version=py_version,\n",
    "            image_scope=\"inference\",\n",
    "            instance_type=instance_type,\n",
    "        )\n",
    "        super().__init__(\n",
    "            model_data, role, entry_point, image_uri=image_uri, framework_version=\"1.8.0\", **kwargs\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate SageMaker Session\n",
    "\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing Arthur packages\n",
    "\n",
    "from arthurai import ArthurAI\n",
    "from arthurai.common.constants import InputType, OutputType, ValueType, Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Connecting to Arthur and instantiating an Arthur model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to Arthur\n",
    "# Please provide the url and api key to your Arthur instance\n",
    "\n",
    "url = ''\n",
    "api_key = ''\n",
    "\n",
    "# credentials are being passed to the client via environment variables\n",
    "connection = ArthurAI(url=url, access_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an Arthur Model Object\n",
    "arthur_model = connection.model(partner_model_id=f\"SageMakerModel_{datetime.now().strftime('%Y%m%d%H%M%S')}\",\n",
    "                                display_name=\"SageMakerArthurGluonDemo1\",\n",
    "                                input_type=InputType.Tabular,\n",
    "                                output_type=OutputType.Multiclass,\n",
    "                                is_batch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 AWS region and  IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the ARN for an AWS IAM role (we recommend with the AmazonSageMakerFullAccess Permission Policy attached)\n",
    "role = ''\n",
    "\n",
    "# To successfully run this notebook, ensure that valid AWS credentials have been set in the shell environment from which this notebook is running\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "region = sagemaker_session._region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 S3 bucket and prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup S3 bucket\n",
    "# You can use a different bucket, but make sure the role you chose for this notebook\n",
    "# has the s3:PutObject permissions. This is the bucket into which the data is captured\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "s3_prefix = f\"autogluon_sm/{sagemaker.utils.sagemaker_timestamp()}\"\n",
    "output_path = f\"s3://{bucket}/{s3_prefix}/output/\"\n",
    "\n",
    "# Data Capture prefixes\n",
    "data_capture_prefix = f\"{s3_prefix}/datacapture\"\n",
    "s3_capture_upload_path = f\"s3://{bucket}/{data_capture_prefix}\"\n",
    "\n",
    "print(f\"Capture path: {s3_capture_upload_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Test access to the S3 bucket\n",
    "Let's quickly verify that the notebook has the right permissions to access the S3 bucket specified above.\n",
    "Upload a simple test object into the S3 bucket.  If this command fails, the data capture and model monitoring capabilities will not work from this notebook.  You can fix this by updating the role associated with this notebook instance to have \"s3:PutObject\" permissions and try this validation again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload a test file\n",
    "S3Uploader.upload_string_as_file_body(body=\"test file\", desired_s3_uri=f\"s3://{bucket}/test_upload\")\n",
    "\n",
    "# Remove from S3 bucket once upload capability is confirmed\n",
    "boto3.resource('s3').Object(bucket, \"test_upload\").delete()\n",
    "\n",
    "print(\"Success! You are all set to proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Deploy pre-trained model with data capture enabled <a id='deploy'></a>\n",
    "\n",
    "In this section, you will upload the pretrained model to the S3 bucket, create an Amazon SageMaker Model, create an Amazon SageMaker real time endpoint, and enable data capture on the endpoint to capture endpoint invocations, predictions, and metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Upload the pre-trained model to S3\n",
    "\n",
    "This code uploads a pre-trained model and gets it ready to deploy. If you already have a pretrained model in Amazon S3, you can add it instead by specifying the s3_key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model data to create your SageMaker model\n",
    "\n",
    "download_model() # Pass in `skip_if_exists=False` to download the model metadata file even if one already exists\n",
    "\n",
    "model_data = sagemaker_session.upload_data(path=str(MODEL_METADATA_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Create SageMaker Model entity\n",
    "\n",
    "This step creates an Amazon SageMaker model from the  model_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.m5.2xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoGluonInferenceModel(\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    region=region,\n",
    "    framework_version=\"0.4\",\n",
    "    py_version=\"py38\",\n",
    "    instance_type=instance_type,\n",
    "    source_dir=\"scripts\",\n",
    "    entry_point=\"tabular_serve.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Deploy the model with data capture enabled.\n",
    "Next, deploy the SageMaker model on a specific instance with data capture enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = sagemaker.utils.unique_name_from_base(\"sagemaker-arthur-integration-test\")\n",
    "\n",
    "# DataCapture Configuration\n",
    "capture_modes = ['REQUEST','RESPONSE']\n",
    "data_capture_config = sagemaker.model_monitor.DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri=s3_capture_upload_path,\n",
    "    capture_options=capture_modes\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1, serializer=CSVSerializer(), instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    data_capture_config=data_capture_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 - Building your Arthur model. <a id='BuildArthur'></a>  \n",
    "\n",
    "Arthur needs a copy of (reference) data in order to, among other purposes, establish a _data schema_ - an expectation of what future data coming into the platform will look like. This schema should include all input features, a field for predictions, and a field for ground truths. If your reference data does not include these columns, it will be necessary to add them before building your Arthur model.\n",
    "\n",
    "**Important: This step needs to be done before you start generating inferences with your SageMaker model, or those inferences will not be logged with Arthur**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Reading in and cleaning up our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in reference_data.csv and format the dataframe for sending to arthur\n",
    "\n",
    "download_reference_dataset()\n",
    "\n",
    "df = pd.read_csv(REFERENCE_DATA_PATH, header=None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first column of this dataset corresponds to the ground truth. \n",
    "# We will actually be removing/setting aside this column for the moment to mimic the state of input data that will be sent to our sagemaker model.\n",
    "\n",
    "testDataTruth = df.iloc[: ,0]\n",
    "df = df.iloc[:,1:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranging Columns to match our SageMaker output\n",
    "\n",
    "df['index1'] = df.index\n",
    "cols = df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Creating/formatting the prediction and ground truth columns (and mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introducing column names for our input features.\n",
    "\n",
    "num_cols = len(list(df))\n",
    "rng = range(1, num_cols+1)\n",
    "colNames = ['Feature_' + str(i) for i in rng]\n",
    "df.columns = colNames\n",
    "\n",
    "# Adding placeholder columns for predictions and ground truth (with their eventual dtypes)...\n",
    "\n",
    "df['prediction_0'], df['prediction_1'] = None, None\n",
    "df['prediction_0'], df['prediction_1'] = df['prediction_0'].astype('float'), df['prediction_1'].astype('float')\n",
    "\n",
    "testDataTruth = testDataTruth.astype(int)\n",
    "df['gt_0'] = 1-testDataTruth\n",
    "df['gt_1'] = testDataTruth\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping between predictions and ground truth.\n",
    "\n",
    "prediction_to_ground_truth_map = {\n",
    "    \"prediction_0\": \"gt_0\",\n",
    "    \"prediction_1\": \"gt_1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Build and review your Arthur Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Building the Arthur model. We provide inputs including: \n",
    "# df - the dataframe which will help establish the data schema of our model\n",
    "# \"ground_truth_column\" - the column which corresponds to our ground truth\n",
    "# \"pred_to_ground_truth_map\" - the mapping which relates predicted probabilities to their corresponding class\n",
    "\n",
    "arthur_model.build(df, pred_to_ground_truth_map=prediction_to_ground_truth_map, positive_predicted_attr=\"prediction_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the arthur_model and returning the model_id\n",
    "\n",
    "model_id = arthur_model.save()\n",
    "print(model_id)\n",
    "\n",
    "with open(\"arthur_model_id.txt\", \"w+\") as f:\n",
    "    f.write(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 4 - Sending inferences through SageMaker and Capturing the Data <a id='SendInferences'></a>  \n",
    "\n",
    "In this section, you will send inferences through the SageMaker endpoint we just created and generate corresponding Data Capture files in S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Generate prediction data\n",
    "\n",
    "The cells below send a small sample of 100 test dataset inferences as to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Format test data to generate predictions\n",
    "\n",
    "download_test_dataset()\n",
    "\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "test_df = test_df.drop(columns=[\"class\"])\n",
    "test_df = test_df.iloc[0:-1, 1:]\n",
    "test_df_sample = test_df[:100]\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "content_type = \"text/csv\"\n",
    "\n",
    "def query_endpoint(encoded_tabular_data):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=content_type, Body=encoded_tabular_data\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_endpoint(test_df_sample.to_csv(header=False).encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 View captured data\n",
    "\n",
    "Now list the data capture files stored in Amazon S3. You should expect to see different files from different time periods organized based on the hour in which the invocation occurred. The format of the Amazon S3 path is:\n",
    "\n",
    "`s3://{destination-bucket-prefix}/{endpoint-name}/{variant-name}/yyyy/mm/dd/hh/filename.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Waiting for captures to show up\", end=\"\")\n",
    "for _ in range(120):\n",
    "    capture_files = sorted(S3Downloader.list(f\"{s3_capture_upload_path}/{endpoint_name}\"))\n",
    "    if capture_files:\n",
    "        capture_file = S3Downloader.read_file(capture_files[-1]).split(\"\\n\")\n",
    "        capture_record = json.loads(capture_file[0])\n",
    "        if \"inferenceId\" in capture_record[\"eventMetadata\"]:\n",
    "            break\n",
    "    print(\".\", end=\"\", flush=True)\n",
    "    sleep(1)\n",
    "print()\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files[-3:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, view the contents of a single capture file. Here you should see all the data captured in an Amazon SageMaker specific JSON-line formatted file. Take a quick peek at the first few lines in the captured file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(capture_file[-3:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the contents of a single line is present below in a formatted JSON file so that you can observe a little better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(capture_record, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Once you've tested your data capture on a sample of your inferences. Set up your Lambda with your saved Arthur model id, then run the cell below to send the remainder of your inferences. Lambda setup documentation for Arthur's SageMaker Integration can be found at [this link](https://docs.arthur.ai/user-guide/integrations.html#aws-lambda-setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_endpoint(test_df.to_csv(header=False).encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
